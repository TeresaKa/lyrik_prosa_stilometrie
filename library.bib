Automatically generated by Mendeley Desktop 1.19.6
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@phdthesis{Bloß2005,
author = {Blo{\ss}, Alexandra},
school = {Julius-Maximilians-Universit{\"{a}}t W{\"{u}}rzburg},
title = {{Merkmale von Figurenrede in narrativen Texten – Ein Vergleich zwischen deutschen und englischen Romanen und Kurzgeschichten}},
year = {2005}
}
@book{Verzani,
abstract = {Includes index.},
address = {Boca Raton},
author = {Verzani, John.},
file = {:home/teresa/Uni/Master/Semester 3/projektseminar/projektliteratur/r{\_}statistics.pdf:pdf},
isbn = {0203499891},
publisher = {Chapman {\&} Hall/CRC},
title = {{Using R for introductory statistics}},
year = {2005}
}
@article{Matthews,
author = {Matthews, Robert and MERRIAM, THOMAS},
doi = {10.1093/llc/8.4.203},
file = {:home/teresa/Uni/Master/Semester 3/projektseminar/projektliteratur/NeuralComputingI-Fletcher.pdf:pdf},
journal = {Literary and Linguistic Computing},
title = {{Neural Computation in Stylometry I: An Application to the Works of Shakespeare and Fletcher}},
volume = {8},
year = {1993}
}
@article{Schaalje,
abstract = {The nearest shrunken centroid (NSC) methodology, originally developed for high-dimensional genomics problems, was recently applied in a stylometric study. Although NSC has many advantages, stylometric problems usually differ from genomics problems in several important ways: texts are of a wide range of sizes, a large series of texts are often the subjects for classification, and most importantly the set of candidate authors cannot usually be assumed to be closed. Consequently, na{\"{i}}ve application of NSC methodology can produce misleading results. We extend the NSC methodology for more general application to stylometry. Reanalysis of the Book of Mormon using the open-set NSC method produced dramatically different results from a closed-set NSC analysis. {\textcopyright} The Author 2011. Published by Oxford University Press on behalf of ALLC, ACH and SDH/SEMI. All rights reserved.},
author = {Schaalje, G. B. and Fields, P. J. and Roper, M. and Snow, G. L.},
doi = {10.1093/llc/fqq029},
file = {:home/teresa/Uni/Master/Semester 3/projektseminar/projektliteratur/extended{\_}neares{\_}shrunken{\_}centroids:},
issn = {0268-1145},
journal = {Literary and Linguistic Computing},
month = {apr},
number = {1},
pages = {71--88},
publisher = {Oxford Academic},
title = {{Extended nearest shrunken centroid classification: A new method for open-set authorship attribution of texts of varying sizes}},
url = {https://academic.oup.com/dsh/article-lookup/doi/10.1093/llc/fqq029},
volume = {26},
year = {2011}
}
@article{Burrows2002,
abstract = {This paper is a companion to my ‘Questions of authorship: attribution and beyond', in which I sketched a new way of using the relative frequencies of the very common words for comparing written texts and testing their likely author- ship. The main emphasis of that paper was not on the new procedure but on the broader consequences of our increasing sophistication in making such com- parisons and the increasing (although never absolute) reliability of our inferences about authorship. My present objects, accordingly, are to give a more complete account of the procedure itself; to report the outcome of an extensive set of trials; and to consider the strengths and limitations of the new procedure. The pro- cedure offers a simple but comparatively accurate addition to our current methods of distinguishing the most likely author of texts exceeding about 1,500 words in length. It is of even greater value as a method of reducing the field of likely candidates for texts of as little as 100 words in length. Not unexpectedly, it works least well with texts of a genre uncharacteristic of their author and, in one case, with texts far separated in time across a long literary career. Its possible use for other classificatory tasks has not yet been investigated.},
author = {Burrows, John},
file = {:home/teresa/Uni/Master/Semester 3/projektseminar/projektliteratur/01{\_}delta.pdf:pdf},
journal = {Literary and Linguistic Computing},
number = {3},
pages = {267--287},
title = {{‘ Delta ': a Measure of Stylistic Authorship}},
volume = {17},
year = {2002}
}
@article{BerenikeHerrmann2015,
abstract = {Language and literary studies have studied style for centuries, and even since the advent of ›stylistics‹ as a discipline at the beginning of the twentieth century, definitions of ›style‹ have varied heavily across time, space and fields. Today, with increasingly large collections of literary texts being made available in digital form, computational approaches to literary style are proliferating. New methods from disciplines such as corpus linguistics and computer science are being adopted and adapted in interrelated fields such as computational stylistics and corpus stylistics, and are facilitating new approaches to literary style.The relation between definitions of style in established linguistic or literary stylistics, and definitions of style in computational or corpus stylistics has not, however, been systematically assessed. This contribution aims to respond to the need to redefine style in the light of this new situation and to establish a clearer perception of both the overlap and the boundaries between ›mainstream‹ and ›computational‹ and/or ›empirical‹ literary stylistics. While stylistic studies of non-literary texts are currently flourishing, our contribution deliberately centers on those approaches relevant to ›literary stylistics‹. It concludes by proposing an operational definition of style that we hope can act as a common ground for diverse approaches to literary style, fostering transdisciplinary research.The focus of this contribution is on literary style in linguistics and literary studies (rather than in art history, musicology or fashion), on textual aspects of style (rather than production- or reception-oriented theories of style), and on a descriptive perspective (rather than a prescriptive or didactic one). Even within these limits, however, it appears necessary to build on a broad understanding of the various perspectives on style that have been adopted at different times and in different traditions. For this reason, the contribution first traces the development of the notion of style in three different traditions, those of German, Dutch and French language and literary studies. Despite the numerous links between each other, and between each of them to the British and American traditions, these three traditions each have their proper dynamics, especially with regard to the convergence and/or confrontation between mainstream and computational stylistics. For reasons of space and coherence, the contribution is limited to theoretical developments occurring since 1945.The contribution begins by briefly outlining the range of definitions of style that can be encountered across traditions today: style as revealing a higher-order aesthetic value, as the holistic ›gestalt‹ of single texts, as an expression of the individuality of an author, as an artifact presupposing choice among alternatives, as a deviation from a norm or reference, or as any formal property of a text. The contribution then traces the development of definitions of style in each of the three traditions mentioned, with the aim of giving a concise account of how, in each tradition, definitions of style have evolved over time, with special regard to the way such definitions relate to empirical, quantitative or otherwise computational approaches to style in literary texts. It will become apparent how, in each of the three traditions, foundational texts continue to influence current discussions on literary style, but also how stylistics has continuously reacted to broader developments in cultural and literary theory, and how empirical, quantitative or computational approaches have long ­existed, usually in parallel to or at the margins of mainstream stylistics. The review will also reflect the lines of discussion around style as a property of literary texts – or of any textual entity in general.The perspective on three stylistic traditions is accompanied by a more systematic perspective. The rationale is to work towards a common ground for literary scholars and linguists when talking about (literary) style, across traditions of stylistics, with respect for established definitions of style, but also in light of the digital paradigm. Here, we first show to what extent, at similar or different moments in time, the three traditions have developed comparable positions on style, and which definitions out of the range of possible definitions have been proposed or promoted by which authors in each of the three traditions.On the basis of this synthesis, we then conclude by proposing an operational definition of style that is an attempt to provide a common ground for both mainstream and computational literary stylistics. This definition is discussed in some detail in order to explain not only what is meant by each term in the definition, but also how it relates to computational analyses of style – and how this definition aims to avoid some of the pitfalls that can be perceived in earlier definitions of style. Our definition, we hope, will be put to use by a new generation of computational, quantitative, and empirical studies of style in literary texts.},
author = {{Berenike Herrmann}, J. and van Dalen-Oskam, Karina and Sch{\"{o}}ch, Christof},
doi = {10.1515/jlt-2015-0003},
file = {:home/teresa/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Berenike Herrmann, van Dalen-Oskam, Sch{\"{o}}ch - 2015 - Revisiting Style, a Key Concept in Literary Studies.pdf:pdf},
issn = {1862-5290},
journal = {Journal of Literary Theory},
number = {1},
pages = {25--52},
title = {{Revisiting Style, a Key Concept in Literary Studies}},
volume = {9},
year = {2015}
}
@article{Argamon2008,
abstract = {While Burrows's intuitive and elegant 'Delta' measure for authorship attribution has proven to be extremely useful for authorship attribution, a theoretical understanding of its operation has remained somewhat obscure. In this article, I address this issue by introducing a geometric interpretation of Delta, which further allows us to interpret Delta as a probabilistic ranking principle. This interpretation gives us a better understanding of the method's fundamental assumptions and potential limitations, as well as leading to several well-founded variations and extensions. {\textcopyright} The Author 2008. Published by Oxford University Press on behalf of ALLC and ACH. All rights reserved.},
author = {Argamon, Shlomo},
doi = {10.1093/llc/fqn003},
file = {:home/teresa/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Argamon - 2008 - Interpreting burrows's delta Geometric and probabilistic foundations.pdf:pdf},
issn = {02681145},
journal = {Literary and Linguistic Computing},
number = {2},
pages = {131--147},
title = {{Interpreting burrows's delta: Geometric and probabilistic foundations}},
volume = {23},
year = {2008}
}
@book{Downey,
abstract = {1. Aufl.},
address = {Beijing ; K{\"{o}}ln [u.a.]},
author = {Downey, Allen. and Beyer, Jörg},
edition = {1. Aufl.},
file = {:home/teresa/Uni/Master/Semester 3/projektseminar/projektliteratur/statistik{\_}workshop.odt:odt},
isbn = {9783868993424},
publisher = {O'Reilly},
title = {{Statistik-Workshop für Programmierer [Einführung in Wahrscheinlichkeit und Statistik ; Statistik verstehen mit Python]}},
year = {2012}
}
@article{Pearl,
abstract = {We describe a new supervised machine learning approach for detecting authorship deception, a specific type of authorship attribution task particularly relevant for cybercrime forensic investigations, and demonstrate its validity on two case studies drawn from realistic online data sets. The core of our approach involves identifying uncharacteristic behavior for an author, based on a writeprint extracted from unstructured text samples of the author's writing. The writeprints used here involve stylometric features and content features derived from topic models, an unsupervised approach for identifying relevant keywords that relate to the content areas of a document. One innovation of our approach is to transform the writeprint feature values into a representation that individually balances characteristic and uncharacteristic traits of an author, and we subsequently apply a Sparse Multinomial Logistic Regression classifier to this novel representation. Our method yields high accuracy for authorship deception detection on the two case studies, confirming its utility. {\textcopyright} The Author 2012. Published by Oxford University Press on behalf of ALLC. All rights reserved.},
author = {Pearl, Lisa and Steyvers, Mark},
doi = {10.1093/llc/fqs003},
file = {:home/teresa/Uni/Master/Semester 3/projektseminar/projektliteratur/PearlSteyvers2012{\_}AuthorshipDeception.pdf:pdf},
issn = {02681145},
journal = {Literary and Linguistic Computing},
month = {jun},
number = {2},
pages = {183--196},
title = {{Detecting authorship deception: A supervised machine learning approach using author writeprints}},
volume = {27},
year = {2012}
}
@misc{piper_2016,
author = {Piper, Andrew},
doi = {10.6084/m9.figshare.2062002.v3},
title = {{txtlab Multilingual Novels}},
url = {https://figshare.com/articles/txtlab{\_}Novel450/2062002/3},
year = {2016}
}
@article{Juola2015,
abstract = {We propose a possible solution to one of the major weaknesses in the application of authorship attribution-the absence of clear-cut standards for accurate analytic practice. To address this, we propose a specific practice as a possible standard and present four recent cases applying this standard. The key elements of this protocol are the use of an ad hoc distractor set in conjunction with multiple analyses structured as a set of elimination tests. This protocol (or close variants of it) has been used in at least four separate cases across a wide variety of documents and consumers. It is mathematically supported while still being easy to understand. We are confident that the proposed protocol will provide a relatively straightforward and understandable way to reduce controversy regarding stylometric authorship attribution, and thereby increase its uptake and credibility.},
author = {Juola, Patrick},
doi = {10.1093/llc/fqv040},
file = {:home/teresa/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Juola - 2015 - The rowling case A proposed standard analytic protocol for authorship questions.pdf:pdf},
issn = {2055768X},
journal = {Digital Scholarship in the Humanities},
pages = {i100--i113},
title = {{The rowling case: A proposed standard analytic protocol for authorship questions}},
volume = {30},
year = {2015}
}
@book{Piegorsch,
abstract = {A comprehensive introduction to statistical methods for data mining and knowledge discovery. Applications of data mining and 'big data' increasingly take center stage in our modern, knowledge-driven society, supported by advances in computing power, automated data acquisition, social media development and interactive, linkable internet software. This book presents a coherent, technical introduction to modern statistical learning and analytics, starting from the core foundations of statistics and probability. It includes an overview of probability and statistical distributions, basic. Cover; Title Page; Copyright; Dedication; Contents; Preface; Part I Background: Introductory Statistical Analytics; Chapter 1 Data analytics and data mining; 1.1 Knowledge discovery: finding structure in data; 1.2 Data quality versus data quantity; 1.3 Statistical modeling versus statistical description; Chapter 2 Basic probability and statistical distributions; 2.1 Concepts in probability; 2.1.1 Probability rules; 2.1.2 Random variables and probability functions; 2.1.3 Means, variances, and expected values; 2.1.4 Median, quartiles, and quantiles. 2.1.5 Bivariate expected values, covariance, and correlation2.2 Multiple random variables*; 2.3 Univariate families of distributions; 2.3.1 Binomial distribution; 2.3.2 Poisson distribution; 2.3.3 Geometric distribution; 2.3.4 Negative binomial distribution; 2.3.5 Discrete uniform distribution; 2.3.6 Continuous uniform distribution; 2.3.7 Exponential distribution; 2.3.8 Gamma and chi-square distributions; 2.3.9 Normal (Gaussian) distribution; 2.3.10 Distributions derived from normal; 2.3.11 The exponential family; Chapter 3 Data manipulation; 3.1 Random sampling; 3.2 Data types. 3.3 Data summarization3.3.1 Means, medians, and central tendency; 3.3.2 Summarizing variation; 3.3.3 Summarizing (bivariate) correlation; 3.4 Data diagnostics and data transformation; 3.4.1 Outlier analysis; 3.4.2 Entropy*; 3.4.3 Data transformation; 3.5 Simple smoothing techniques; 3.5.1 Binning; 3.5.2 Moving averages*; 3.5.3 Exponential smoothing*; Chapter 4 Data visualization and statistical graphics; 4.1 Univariate visualization; 4.1.1 Strip charts and dot plots; 4.1.2 Boxplots; 4.1.3 Stem-and-leaf plots; 4.1.4 Histograms and density estimators; 4.1.5 Quantile plots. 4.2 Bivariate and multivariate visualization4.2.1 Pie charts and bar charts; 4.2.2 Multiple boxplots and QQ plots; 4.2.3 Scatterplots and bubble plots; 4.2.4 Heatmaps; 4.2.5 Time series plots*; Chapter 5 Statistical inference; 5.1 Parameters and likelihood; 5.2 Point estimation; 5.2.1 Bias; 5.2.2 The method of moments; 5.2.3 Least squares/weighted least squares; 5.2.4 Maximum likelihood*; 5.3 Interval estimation; 5.3.1 Confidence intervals; 5.3.2 Single-sample intervals for normal (Gaussian) parameters; 5.3.3 Two-sample intervals for normal (Gaussian) parameters. 5.3.4 Wald intervals and likelihood intervals*5.3.5 Delta method intervals*; 5.3.6 Bootstrap intervals*; 5.4 Testing hypotheses; 5.4.1 Single-sample tests for normal (Gaussian) parameters; 5.4.2 Two-sample tests for normal (Gaussian) parameters; 5.4.3 Walds tests, likelihood ratio tests, and 'exact' tests*; 5.5 Multiple inferences*; 5.5.1 Bonferroni multiplicity adjustment; 5.5.2 False discovery rate; Part II Statistical Learning and Data Analytics; Chapter 6 Techniques for supervised learning: simple linear regression; 6.1 What is ""supervised learning?""; 6.2 Simple linear regression.},
address = {Chichester},
author = {Piegorsch, Walter W.},
file = {:home/teresa/Uni/Master/Semester 3/projektseminar/projektliteratur/statisticaldataanalytics.odt:odt},
isbn = {9781118619650},
publisher = {Wiley},
title = {{Statistical data analytics : foundations for data mining, informatics, and knowledge discovery}},
year = {2015}
}
@article{KleinginnaPaulR;Kleinginna1981,
author = {{Kleinginna, Paul R; Kleinginna}, Anne M.},
journal = {Motivation and Emotion 4},
title = {{A categorized list of emotion definitions, with suggestions for a consensual definition.}},
year = {1981}
}
@article{Evert2017,
abstract = {This article builds on a mathematical explanation of one the most prominent stylometric measures, Burrows's Delta (and its variants), to understand and explain its working. Starting with the conceptual separation between feature selection, feature scaling, and distance measures, we have designed a series of controlled experiments in which we used the kind of feature scaling (various types of standardization and normalization) and the type of distance measures (notably Manhattan, Euclidean, and Cosine) as independent variables and the correct authorship attributions as the dependent variable indicative of the performance of each of the methods proposed. In this way, we are able to describe in some detail how each of these two variables interact with each other and how they influence the results. Thus we can show that feature vector normalization, that is, the transformation of the feature vectors to a uniform length of 1 (implicit in the cosine measure), is the decisive factor for the improvement of Delta proposed recently. We are also able to show that the information particularly relevant to the identification of the author of a text lies in the profile of deviation across the most frequent words rather than in the extent of the deviation or in the deviation of specific words only.},
author = {Evert, Stefan and Proisl, Thomas and Jannidis, Fotis and Reger, Isabella and Pielstr{\"{o}}m, Steffen and Sch{\"{o}}ch, Christof and Vitt, Thorsten},
doi = {10.1093/llc/fqx023},
file = {:home/teresa/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Evert et al. - 2017 - Understanding and explaining Delta measures for authorship attribution.pdf:pdf},
issn = {2055768X},
journal = {Digital Scholarship in the Humanities},
number = {June},
pages = {ii4--ii16},
title = {{Understanding and explaining Delta measures for authorship attribution}},
volume = {32},
year = {2017}
}
@book{Hillebrandt2011,
address = {Berlin},
author = {Hillebrandt, Claudia},
doi = {10.1524/9783050057163},
isbn = {978-3-05-005196-3},
month = {jan},
publisher = {Akademie Verlag},
title = {{Das emotionale Wirkungspotenzial von Erz{\"{a}}hltexten}},
url = {http://www.degruyter.com/doi/book/10.1524/9783050057163},
year = {2011}
}
@book{Krah2006,
address = {Kiel},
author = {Krah, Hans},
publisher = {Ludwig},
title = {{Einf{\"{u}}hrung in die Literaturwissenschaft. Textanalyse}},
year = {2006}
}
@article{Smith2011,
author = {Smith, Peter W. H. and Aldridge, W.},
doi = {10.1080/09296174.2011.533591},
issn = {0929-6174},
journal = {Journal of Quantitative Linguistics},
month = {feb},
number = {1},
pages = {63--88},
title = {{Improving Authorship Attribution: Optimizing Burrows' Delta Method*}},
url = {http://www.tandfonline.com/doi/abs/10.1080/09296174.2011.533591},
volume = {18},
year = {2011}
}
@article{Burrows2003,
author = {Burrows, John},
issn = {00104817},
journal = {Computers and the Humanities},
number = {1},
pages = {5--32},
publisher = {Springer},
title = {{Questions of Authorship: Attribution and Beyond: A Lecture Delivered on the Occasion of the Roberto Busa Award ACH-ALLC 2001, New York}},
url = {http://www.jstor.org/stable/30204877},
volume = {37},
year = {2003}
}
@article{Tearle,
abstract = {We present an algorithm as evidence of the possibility of a truly automated stylometric authorship attribution tool, based on committees of artificial neural networks. Neural networks have an advantage over traditional statistical stylometry in that they are inherently nonlinear, and therefore can consider nonlinear interactions between stylometric variables. The algorithm presented (1) is intended to demonstrate the feasibility of an automated approach using neural networks and (2) highlights important areas for further research. We present results of two separate test experiments - Shakespeare and Marlowe, and the Federalist Papers - as a demonstration of the method's; generality. In both cases, our algorithm produces committees that correctly predict the test works, without requiring the usual precursory statistical study to determine efficacious stylometric measures. {\textcopyright} The Author 2008. Published by Oxford University Press on behalf of ALLC and ACH. All rights reserved.},
author = {Tearle, M. and Taylor, K. and Demuth, H.},
doi = {10.1093/llc/fqn022},
file = {:home/teresa/Uni/Master/Semester 3/projektseminar/projektliteratur/attribution{\_}neural{\_}net.pdf:pdf},
issn = {0268-1145},
journal = {Literary and Linguistic Computing},
month = {sep},
number = {4},
pages = {425--442},
publisher = {Oxford Academic},
title = {{An algorithm for automated authorship attribution using neural networks}},
url = {https://academic.oup.com/dsh/article-lookup/doi/10.1093/llc/fqn022},
volume = {23},
year = {2008}
}
@book{McKinney,
abstract = {Python for Data Analysis is concerned with the nuts and bolts of manipulating, processing, cleaning, and crunching data in Python. It is also a practical, modern introduction to scientific computing in Python, tailored for data-intensive applications. This is a book about the parts of the Python language and libraries you'll need to effectively solve a broad set of data analysis problems. This book is not an exposition on analytical methods using Python as the implementation language. Written by Wes McKinney, the main author of the pandas library, this hands-on book is packed with practical cases studies. It's ideal for analysts new to Python and for Python programmers new to scientific computing. Use the IPython interactive shell as your primary development environment Learn basic and advanced NumPy (Numerical Python) features Get started with data analysis tools in the pandas library Use high-performance tools to load, clean, transform, merge, and reshape data Create scatter plots and static or interactive visualizations with matplotlib Apply the pandas groupby facility to slice, dice, and summarize datasets Measure data by points in time, whether it's specific instances, fixed periods, or intervals Learn how to solve problems in web analytics, social sciences, finance, and economics, through detailed examples},
address = {Beijing ; Boston ; Farnham ; Sebastopol ; Tokyo},
author = {McKinney, Wes},
booktitle = {O'Reilly Media, Inc.},
edition = {Second edi},
isbn = {9781491957660},
number = {5},
pages = {358--359},
publisher = {O'Reilly},
title = {{Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython}},
volume = {15},
year = {1976}
}
@misc{maciej,
author = {{Eder, M., Rybicki, J. and Kestemont}, M.},
booktitle = {R Journal, 8(1)},
pages = {107--21},
title = {{Stylometry with R: a package for computational text analysis.}},
url = {https://journal.r-project.org/archive/2016/RJ-2016-007/index.html},
year = {2016}
}
@misc{refcor,
address = {W{\"{u}}rzburg},
author = {Computerphilologie},
title = {{Refcor}},
url = {https://github.com/cophi-wue/refcor},
year = {2017}
}
@book{Stocker2016,
address = {Berlin, Boston},
author = {Stocker, Toni C. and Steinke, Ingo},
doi = {10.1515/9783110353891},
isbn = {9783110353891},
month = {jan},
publisher = {De Gruyter},
title = {{Statistik}},
url = {https://www.degruyter.com/view/books/9783110353891/9783110353891/9783110353891.xml},
year = {2016}
}
@article{Jockers,
author = {Jockers, Matthew and Witten, Daniela},
doi = {10.1093/llc/fqq001},
file = {:home/teresa/Uni/Master/Semester 3/projektseminar/projektliteratur/ML{\_}comparative{\_}study.pdf:pdf},
journal = {LLC},
pages = {215--223},
title = {{A comparative study of machine learning methods for authorship attribution}},
volume = {25},
year = {2010}
}
@article{Stamatatos,
abstract = {Authorship attribution supported by statistical or computational methods has a long history starting from the 19th century and is marked by the seminal study of Mosteller and Wallace (1964) on the authorship of the disputed "Federalist Papers." During the last decade, this scientific field has been developed substantially, taking advantage of research advances in areas such as machine learning, information retrieval, and natural language processing. The plethora of available electronic texts (e.g., e-mail messages, online forum messages, blogs, source code, etc.) indicates a wide variety of applications of this technology, provided it is able to handle short and noisy text from multiple candidate authors. In this article, a survey of recent advances of the automated approaches to attributing authorship is presented, examining their characteristics for both text representation and text classification. The focus of this survey is on computational requirements and settings rather than on linguistic or literary issues. We also discuss evaluation methodologies and criteria for authorship attribution studies and list open questions that will attract future work in this area.},
author = {Stamatatos, Efstathios},
doi = {10.1002/asi.21001},
file = {:home/teresa/Uni/Master/Semester 3/projektseminar/projektliteratur/survey{\_}of{\_}methods.pdf:pdf},
issn = {15322882},
journal = {Journal of the American Society for Information Science and Technology},
month = {mar},
number = {3},
pages = {538--556},
title = {{A survey of modern authorship attribution methods}},
volume = {60},
year = {2009}
}
@book{Kuckartz,
abstract = {Dieses Lehrbuch der statistischen Datenanalyse wurde speziell f{\"{u}}r Einf{\"{u}}hrungskurse konzipiert und richtet sich an alle, die eine leicht verst{\"{a}}ndliche Einf{\"{u}}hrung in die sozialwissenschaftliche Statistik suchen. Es bezieht sich auf das Feld der Erziehungs- und Sozialwissenschaften und behandelt den Stoff nicht als inhaltsunabh{\"{a}}ngiges mathematisches Wissen.},
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {Kuckartz, Udo},
doi = {10.1007/978-3-531-19890-3},
eprint = {9809069v1},
file = {:home/teresa/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kuckartz - 2013 - Statistik eine verst{\"{a}}ndliche Einf{\"{u}}hrung.pdf:pdf;:home/teresa/Uni/Master/Semester 3/projektseminar/projektliteratur/Kuckartz2013{\_}Chapter{\_}MittelwerteUndStreuungsma{\ss}e.pdf:pdf;:home/teresa/Uni/Master/Semester 3/projektseminar/projektliteratur/Kuckartz2013{\_}Chapter{\_}VarianzanalyseMehrAlsZweiMitte.pdf:pdf},
isbn = {3531198904},
issn = {0717-6163},
pages = {313},
pmid = {15003161},
primaryClass = {arXiv:gr-qc},
publisher = {Springer VS},
title = {{Statistik: eine verst{\"{a}}ndliche Einf{\"{u}}hrung}},
volume = {91},
year = {2013}
}
@article{Evert2017a,
author = {Evert, Stefan and Proisl, Thomas and Jannidis, Fotis and Reger, Isabella and Pielstr{\"{o}}m, Steffen and Sch{\"{o}}ch, Christof and Vitt, Thorsten},
doi = {10.1093/llc/fqx023},
issn = {2055-7671},
journal = {Digital Scholarship in the Humanities},
month = {dec},
number = {suppl{\_}2},
pages = {ii4--ii16},
title = {{Understanding and explaining Delta measures for authorship attribution}},
url = {http://academic.oup.com/dsh/article/32/suppl{\_}2/ii4/3865676},
volume = {32},
year = {2017}
}
